from __gin__ import dynamic_registration
import __main__ as train_script

import optax

from t5x import utils
from t5x import trainer

from lang_transfer import tasks
from lang_transfer import preprocessing

include "t5x/configs/runs/pretrain.gin"
include "lang_transfer/configs/models/70M.gin"

# REQUIRED variables
EVAL_PERIOD = %gin.REQUIRED

# PREDEFINED variables
BATCH_SIZE = 512
MICRO_BATCHES = 4
TASK_FEATURE_LENGTHS={"targets":1024}
EVAL_STEPS = 550 # to run the validation over aprox 288M tokens 
PRETRAINED_MODEL_PATH = []  # From scratch
VAL_MIXTURE_OR_TASK_NAME = %MIXTURE_OR_TASK_NAME
TRAIN_DATASET_EPOCHS = None

DROPOUT_RATE = 0.0
RANDOM_SEED = 42

# DATASET overrides
train/utils.DatasetConfig:
  seed = %RANDOM_SEED

train_eval/utils.DatasetConfig:
  mixture_or_task_name = %VAL_MIXTURE_OR_TASK_NAME

# TRAIN script overrides
train_script.train.eval_steps = %EVAL_STEPS
train_script.train.eval_period = %EVAL_PERIOD
train_script.train.get_dataset_fn = @utils.get_dataset

utils.get_dataset:
  num_epochs = %TRAIN_DATASET_EPOCHS

# MICRO-BATCHES & LR SCHEDULER configuration
trainer.Trainer:
  num_microbatches = %MICRO_BATCHES
  learning_rate_fn = @optax.warmup_cosine_decay_schedule()

optax.warmup_cosine_decay_schedule:
  init_value = 0.0
  peak_value = 2e-4
  end_value = 2e-5
  warmup_steps = %WARMUP_STEPS
  decay_steps = %TRAIN_STEPS

# CHECKPOINT Config
utils.SaveCheckpointConfig:
  period = %EVAL_PERIOD
  save_dataset = True

